var documenterSearchIndex = {"docs":
[{"location":"examples/mpi/#MPI.jl-Integration","page":"MPI.jl Integration","title":"MPI.jl Integration","text":"","category":"section"},{"location":"examples/mpi/","page":"MPI.jl Integration","title":"MPI.jl Integration","text":"For this example we will use the FluxMPI.jl package which adds Multi-GPU/Node Training support for Flux.jl using MPI.jl","category":"page"},{"location":"examples/mpi/","page":"MPI.jl Integration","title":"MPI.jl Integration","text":"using Flux, FluxMPI, MPI, Zygote, CUDA, Wandb, Dates\n\nMPI.Init()\nCUDA.allowscalar(false)\n\nlg = WandbLoggerMPI(project = \"Wandb.jl\",\n                    name = \"mpijl-demo-$(now())\")\n\ntotal_gpus = length(CUDA.devices())\ncomm = MPI.COMM_WORLD\nrank = MPI.Comm_rank(comm)\nsize = MPI.Comm_size(comm)\n\nmodel = Chain(Dense(1, 2, tanh), Dense(2, 1))\n\nmodel_dp = DataParallelFluxModel(\n    model,\n    [i % total_gpus for i = 1:MPI.Comm_size(MPI.COMM_WORLD)],\n)\n\nps = Flux.params(model_dp)\n\nx = rand(1, 64) |> gpu\ny = x .^ 2\n\ndataloader = DataParallelDataLoader((x, y), batchsize = 16)\n\nfunction loss(x_, y_)\n    loss = sum(abs2, model_dp(x_) .- y_)\n    Zygote.@ignore log(lg, Dict(\"loss\" => loss))\n    return loss\nend\n\nfor epoch in 1:100\n    if rank == 0\n        @info \"epoch = $epoch\" \n    end\n    Flux.Optimise.train!(loss, ps, dataloader, Flux.ADAM(0.001))\nend","category":"page"},{"location":"examples/mpi/","page":"MPI.jl Integration","title":"MPI.jl Integration","text":"The main points when using MPI and Wandb are:","category":"page"},{"location":"examples/mpi/","page":"MPI.jl Integration","title":"MPI.jl Integration","text":"MPI.Init() must be called before WandbLoggerMPI is called.\nThe config cannot be updated after WandbLoggerMPI is initialized, i.e. update_config! won't work.\nLogging is done the following manner:\nIf group kwarg is not passed/ is nothing: All the logging is done by the process with rank = 0.\nIf group is set to a string: Look at https://docs.wandb.ai/guides/track/advanced/grouping for more details. (The rank of the processes are appended to the name if set)","category":"page"},{"location":"examples/mpi/","page":"MPI.jl Integration","title":"MPI.jl Integration","text":"The code should be run using mpiexecjl -n 3 julia <script>.jl.","category":"page"},{"location":"examples/artifacts/#Wandb-Artifacts","page":"Artifacts API","title":"Wandb Artifacts","text":"","category":"section"},{"location":"examples/artifacts/","page":"Artifacts API","title":"Artifacts API","text":"All the methods listed in https://docs.wandb.ai/ref/python/artifact can be used by passing a WandbArtifact instance as the first argument. Additionally, log should be used instead of log_artifact function.","category":"page"},{"location":"examples/artifacts/","page":"Artifacts API","title":"Artifacts API","text":"NOTE: These functions are not exported. So use them as Wandb.<function>(::WandbArtifact, ...)","category":"page"},{"location":"examples/artifacts/#Example-Usage:","page":"Artifacts API","title":"Example Usage:","text":"","category":"section"},{"location":"examples/artifacts/","page":"Artifacts API","title":"Artifacts API","text":"using Wandb\n\nlg = WandbLogger(project = \"Wandb.jl\")\nwa = WandbArtifact(\"some-dataset\", type = \"dataset\")\nWandb.add_file(\"a.txt\")\n\nlog(lg, wa)\n\nclose(lg)","category":"page"},{"location":"examples/artifacts/","page":"Artifacts API","title":"Artifacts API","text":"When uploading large artifacts it might be difficult to verify if the files are actually being uploaded or if the code is just stuck. Run the following in your terminal replacing $WANDB_DIR with the path to the local wandb directory:","category":"page"},{"location":"examples/artifacts/","page":"Artifacts API","title":"Artifacts API","text":"tail -f $WANDB_DIR/latest_run/debug-internal.log","category":"page"},{"location":"examples/artifacts/","page":"Artifacts API","title":"Artifacts API","text":"If you have another wandb run started after this, you need to modify the path accordingly.","category":"page"},{"location":"misc/#Miscellaneous-Stuff","page":"Miscellaneous","title":"Miscellaneous Stuff","text":"","category":"section"},{"location":"misc/#Available-Logging-Objects","page":"Miscellaneous","title":"Available Logging Objects","text":"","category":"section"},{"location":"misc/","page":"Miscellaneous","title":"Miscellaneous","text":"Image\nVideo\nHistogram\nObject3D\nprecision_recall","category":"page"},{"location":"misc/","page":"Miscellaneous","title":"Miscellaneous","text":"NOTE: These are not exported since these names are too generic.","category":"page"},{"location":"misc/","page":"Miscellaneous","title":"Miscellaneous","text":"","category":"page"},{"location":"misc/#Third-Party-Integrations","page":"Miscellaneous","title":"Third Party Integrations","text":"","category":"section"},{"location":"misc/","page":"Miscellaneous","title":"Miscellaneous","text":"FluxTraining.jl –> WandbBackend\nMPI –> WandbLoggerMPI","category":"page"},{"location":"misc/","page":"Miscellaneous","title":"Miscellaneous","text":"","category":"page"},{"location":"misc/#Using-Undocumented-Features","page":"Miscellaneous","title":"Using Undocumented Features","text":"","category":"section"},{"location":"misc/","page":"Miscellaneous","title":"Miscellaneous","text":"Most of the wandb API should be usable through Wandb.wandb. In case something isn't working as expected, open an Issue/PR.","category":"page"},{"location":"misc/","page":"Miscellaneous","title":"Miscellaneous","text":"","category":"page"},{"location":"misc/#Troubleshooting","page":"Miscellaneous","title":"Troubleshooting","text":"","category":"section"},{"location":"misc/","page":"Miscellaneous","title":"Miscellaneous","text":"Wandb.jl crashed Julia :( / Long error messages but stuff works :O","category":"page"},{"location":"misc/","page":"Miscellaneous","title":"Miscellaneous","text":"It is possible that when creating the first logger instance wandb crashes julia or throws a verbose error about SSL not being present. A workaround to this is to use the env variable LD_PRELOAD and point it to the libssl.so library shipped by conda. For me it is present in /mnt/miniconda3/lib/libssl.so. If you are using Conda.jl installation with ENV[\"PYTHON\"] = \"\" then it should be present inside ~/.julia/conda/....","category":"page"},{"location":"misc/","page":"Miscellaneous","title":"Miscellaneous","text":"Here is a documented issue in PyCall.jl.","category":"page"},{"location":"misc/","page":"Miscellaneous","title":"Miscellaneous","text":"P.S. If anyone knows a proper solution to this problem, please open a PR to update this section.","category":"page"},{"location":"misc/","page":"Miscellaneous","title":"Miscellaneous","text":"Can't use Wandb.Histogram / FluxTraining.LogHistograms","category":"page"},{"location":"misc/","page":"Miscellaneous","title":"Miscellaneous","text":"Install numpy. Make sure to install in the same environment as PyCall.jl. This should do it","category":"page"},{"location":"misc/","page":"Miscellaneous","title":"Miscellaneous","text":"using PyCall\nrun(`$(PyCall.pyprogramname) -m pip install numpy`)","category":"page"},{"location":"misc/","page":"Miscellaneous","title":"Miscellaneous","text":"Can't use Wandb.Image / FluxTraining.LogVisualization","category":"page"},{"location":"misc/","page":"Miscellaneous","title":"Miscellaneous","text":"Install pillow. Make sure to install in the same environment as PyCall.jl. This should do it","category":"page"},{"location":"misc/","page":"Miscellaneous","title":"Miscellaneous","text":"using PyCall\nrun(`$(PyCall.pyprogramname) -m pip install pillow`)","category":"page"},{"location":"misc/","page":"Miscellaneous","title":"Miscellaneous","text":"","category":"page"},{"location":"quickstart/#Quick-Start","page":"QuickStart","title":"Quick Start","text":"","category":"section"},{"location":"quickstart/","page":"QuickStart","title":"QuickStart","text":"Follow the quickstart points 1 and 2 to get started with a Wandb account.","category":"page"},{"location":"quickstart/","page":"QuickStart","title":"QuickStart","text":"# Initialize the project\nlg = WandbLogger(project = \"Wandb.jl\", name = nothing)\n\n# Set logger globally / in scope / in combination with other loggers\nglobal_logger(lg)\n\n# Logging Values\nlog(lg, Dict(\"accuracy\" => 0.9, \"loss\" => 0.3))\n\n# Even more conveniently\n@info \"metrics\" accuracy=0.9 loss=0.3\n@debug \"metrics\" not_print=-1  # Will have to change debug level for this to be logged\n\n# Tracking Hyperparameters\nupdate_config!(lg, Dict(\"dropout\" => 0.2))\n\n# Close the logger\nclose(lg)  # `finish` works as well but is not a recommended api","category":"page"},{"location":"examples/fluxtraining/#Integration-with-FluxTraining.jl","page":"FluxTraining.jl Integration","title":"Integration with FluxTraining.jl","text":"","category":"section"},{"location":"examples/fluxtraining/","page":"FluxTraining.jl Integration","title":"FluxTraining.jl Integration","text":"We have bindings in the form of WandbBackend which can be used as a dropin replacement for the default TensorboardBackend. Just ensure that FluxTraining.jl is installed and loaded explicitly.","category":"page"},{"location":"examples/hparams/#Hyperparameter-Sweeps","page":"HyperParameter Sweeps","title":"Hyperparameter Sweeps","text":"","category":"section"},{"location":"examples/hparams/","page":"HyperParameter Sweeps","title":"HyperParameter Sweeps","text":"We currently don't support Wandb Agents API since it leads to segfaults. Instead we recommend users to install Hyperopt.jl or any other HyperParameter Optimization Library. The resultant Wandb logs aren't as neat as the official sweeps but do get the job done.","category":"page"},{"location":"examples/hparams/","page":"HyperParameter Sweeps","title":"HyperParameter Sweeps","text":"using Hyperopt\nusing Wandb\n\nf(x, a, b; c) =\n    sum(@. x + (a - 3) ^ 2 + (b ? 10 : 20) + (c - 100) ^ 2) # Function to minimize\n\n# This function dispatch must be present\n# `lg` can be `WandbBackend` if using that with HyperParameter Sweep\nfunction f(lg::WandbLogger, config::Dict)\n    res = f(config[\"x\"], config[\"a\"], config[\"b\"]; c = config[\"c\"])\n    log(lg, Dict(\"result\" => res))\n    return res\nend\n\nhpsweep = WandbHyperParameterSweep()\n\n# Main macro. The first argument to the for loop is always interpreted as the number of iterations\nho = @hyperopt for i=50,\n            sampler = RandomSampler(), # This is default if none provided\n            a = LinRange(1,5,1000),\n            b = [true, false],\n            c = exp10.(LinRange(-1,3,1000))\n    hpsweep(f, Dict(\"a\" => a, \"b\" => b, \"c\" => c),\n            project = \"Wandb.jl\",\n            config = Dict(\"x\" => 100))\nend","category":"page"},{"location":"examples/hparams/","page":"HyperParameter Sweeps","title":"HyperParameter Sweeps","text":"After this is done, we need to do some manual tweaking in the Wandb UI to get a clean visualization. First, filter the runs using the tag in hpsweep. Then just add a Parallel Coordinates plot with the hyperparameters.","category":"page"},{"location":"examples/hparams/","page":"HyperParameter Sweeps","title":"HyperParameter Sweeps","text":"(Image: Parallel Coordinates Plot)","category":"page"},{"location":"examples/demo/#Getting-Started-Tutorial","page":"Getting Started","title":"Getting Started Tutorial","text":"","category":"section"},{"location":"examples/demo/","page":"Getting Started","title":"Getting Started","text":"Example borrowed from here","category":"page"},{"location":"examples/demo/","page":"Getting Started","title":"Getting Started","text":"using Wandb, Dates, Logging\n\n# Start a new run, tracking hyperparameters in config\nlg = WandbLogger(project = \"Wandb.jl\",\n                 name = \"wandbjl-demo-$(now())\",\n                 config = Dict(\"learning_rate\" => 0.01,\n                               \"dropout\" => 0.2,\n                               \"architecture\" => \"CNN\",\n                               \"dataset\" => \"CIFAR-100\"))\n\n# Use LoggingExtras.jl to log to multiple loggers together\nglobal_logger(lg)\n\n# Simulating the training or evaluation loop\nfor x ∈ 1:50\n    acc = log(1 + x + rand() * get_config(lg, \"learning_rate\") + rand() + get_config(lg, \"dropout\"))\n    loss = 10 - log(1 + x + rand() + x * get_config(lg, \"learning_rate\") + rand() + get_config(lg, \"dropout\"))\n    # Log metrics from your script to W&B\n    @info \"metrics\" accuracy=acc loss=loss\nend\n\n# Finish the run\nclose(lg)","category":"page"},{"location":"examples/flux/#Integration-with-Flux.jl","page":"Flux.jl Intergration","title":"Integration with Flux.jl","text":"","category":"section"},{"location":"examples/flux/","page":"Flux.jl Intergration","title":"Flux.jl Intergration","text":"Using Wandb.jl in existing Flux workflows is pretty easy. Let's go through the mp_mnist demo in Flux model-zoo and update it to use Wandb. Firstly, use this evironment and add Wandb.jl to it.","category":"page"},{"location":"examples/flux/","page":"Flux.jl Intergration","title":"Flux.jl Intergration","text":"using Flux, Statistics\nusing Flux.Data: DataLoader\nusing Flux: onehotbatch, onecold, @epochs\nusing Flux.Losses: logitcrossentropy\nusing CUDA\nusing MLDatasets\nusing Wandb\nusing Dates\n\nlg = WandbLogger(\n    project = \"Wandb.jl\",\n    name = \"fluxjl-integration-$(now())\",\n    config = Dict(\n        \"learning_rate\" => 3e-4,\n        \"batchsize\" => 256,\n        \"epochs\" => 100,\n        \"dataset\" => \"MNIST\",\n        \"use_cuda\" => true,\n    ),\n)\n\nglobal_logger(lg)\n\n##################################################################################\n# Wandb # Instead of passing arguments around we will use the global configuration\n# Wandb # file from Wandb\n##################################################################################\nfunction getdata(device)\n    ENV[\"DATADEPS_ALWAYS_ACCEPT\"] = \"true\"\n\n    # Loading Dataset\t\n    xtrain, ytrain = MLDatasets.MNIST.traindata(Float32)\n    xtest, ytest = MLDatasets.MNIST.testdata(Float32)\n\n    # Reshape Data in order to flatten each image into a linear array\n    xtrain = Flux.flatten(xtrain)\n    xtest = Flux.flatten(xtest)\n\n    # One-hot-encode the labels\n    ytrain, ytest = onehotbatch(ytrain, 0:9), onehotbatch(ytest, 0:9)\n\n    # Create DataLoaders (mini-batch iterators)\n    train_loader = DataLoader(\n        (xtrain, ytrain),\n        batchsize = get_config(lg, \"batchsize\"),\n        shuffle = true,\n    )\n    test_loader = DataLoader((xtest, ytest), batchsize = get_config(lg, \"batchsize\"))\n\n    return train_loader, test_loader\nend\n\nbuild_model(; imgsize = (28, 28, 1), nclasses = 10) =\n    Chain(Dense(prod(imgsize), 32, relu), Dense(32, nclasses))\n\nfunction loss_and_accuracy(data_loader, model, device)\n    acc = 0\n    ls = 0.0f0\n    num = 0\n    for (x, y) in data_loader\n        x, y = device(x), device(y)\n        ŷ = model(x)\n        ls += logitcrossentropy(model(x), y, agg = sum)\n        acc += sum(onecold(cpu(model(x))) .== onecold(cpu(y)))\n        num += size(x, 2)\n    end\n    return ls / num, acc / num\nend\n\n#################################################################\n# Wandb # If any paramters need to be updated pass them as a Dict\n#################################################################\nfunction train(update_params::Dict = Dict())\n    #################################\n    # Wandb # Update config if needed\n    #################################\n    update_config!(lg, update_params)\n\n    if CUDA.functional() && wandb_get_config(\"use_cuda\")\n        @info \"Training on CUDA GPU\"\n        CUDA.allowscalar(false)\n        device = gpu\n    else\n        @info \"Training on CPU\"\n        device = cpu\n    end\n\n    # Create test and train dataloaders\n    train_loader, test_loader = getdata(device)\n\n    # Construct model\n    model = build_model() |> device\n    ps = Flux.params(model) # model's trainable parameters\n\n    ## Optimizer\n    opt = ADAM(get_config(lg, \"learning_rate\"))\n\n    ## Training\n    for epoch = 1:get_config(lg, \"epochs\")\n        for (x, y) in train_loader\n            x, y = device(x), device(y) # transfer data to device\n            gs = gradient(() -> logitcrossentropy(model(x), y), ps) # compute gradient\n            Flux.Optimise.update!(opt, ps, gs) # update parameters\n        end\n\n        # Report on train and test\n        train_loss, train_acc = loss_and_accuracy(train_loader, model, device)\n        test_loss, test_acc = loss_and_accuracy(test_loader, model, device)\n\n        ###################################\n        # Wandb # Log the loss and accuracy\n        ###################################\n        log(\n            lg,\n            Dict(\n                \"Training/Loss\" => train_loss,\n                \"Training/Accuracy\" => train_acc,\n                \"Testing/Loss\" => test_loss,\n                \"Testing/Accuracy\" => test_acc,\n            ),\n        )\n\n        println(\"Epoch=$epoch\")\n        println(\"  train_loss = $train_loss, train_accuracy = $train_acc\")\n        println(\"  test_loss = $test_loss, test_accuracy = $test_acc\")\n    end\nend\n\n### Run training \ntrain()\n\n################################\n# Wandb # Finish the Current Run\n################################\nclose(lg)","category":"page"},{"location":"#Wandb.jl","page":"Home","title":"Wandb.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Unofficial Julia Bindings for wandb.ai.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"For stable release:","category":"page"},{"location":"","page":"Home","title":"Home","text":"] add Wandb","category":"page"},{"location":"","page":"Home","title":"Home","text":"For the main branch:","category":"page"},{"location":"","page":"Home","title":"Home","text":"] add Wandb#main","category":"page"},{"location":"#Examples","page":"Home","title":"Examples","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To see the logging in action go here. Detailed code for these examples can be accessed via the navigation menu.","category":"page"},{"location":"#Running-into-Issues","page":"Home","title":"Running into Issues","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Please have a look at the Miscellaneous Section to see if it solves your issue. If not, please report bugs using GitHub Issues. For usage questions post them on Discourse (@avik-pal) or Julia Slack (#helpdesk channel) (@avikpal) tagging me.","category":"page"}]
}
