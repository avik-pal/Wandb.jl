var documenterSearchIndex = {"docs":
[{"location":"examples/mpi/#MPI.jl-Integration","page":"MPI.jl Integration","title":"MPI.jl Integration","text":"","category":"section"},{"location":"examples/mpi/","page":"MPI.jl Integration","title":"MPI.jl Integration","text":"For this example we will use the FluxMPI.jl package which adds Multi-GPU/Node Training support for Flux.jl using MPI.jl","category":"page"},{"location":"examples/mpi/","page":"MPI.jl Integration","title":"MPI.jl Integration","text":"using Flux, FluxMPI, CUDA, Dates, Wandb,  Zygote\n\n# Step 1: Initialize FluxMPI. Not doing this will segfault your code\nFluxMPI.Init()\nCUDA.allowscalar(false)\n\nlg = WandbLoggerMPI(project = \"Wandb.jl\",\n                    name = \"mpijl-demo-$(now())\")\n\n# Step 2: Sync Model Parameters\nmodel = Chain(Dense(1, 2, tanh), Dense(2, 1)) |> gpu\nps = Flux.params(model)\nFluxMPI.synchronize!(model; root_rank = 0)\n\n# It is the user's responsibility to partition the data across the processes\n# In this case, we are training on a total of 16 * <np> samples\nx = rand(1, 16) |> gpu\ny = x .^ 2\ndataloader = Flux.DataLoader((x, y), batchsize = 16)\n\n# Step 3: Wrap the optimizer in DistributedOptimizer\n#         Scale the learning rate by the number of workers (`total_workers()`).\nopt = Flux.ADAM(0.001)\n\nfunction loss(x_, y_)\n    l = sum(abs2, model(x_) .- y_)\n    Zygote.@ignore Wandb.log(lg, Dict(\"loss\" => l))\n    return l\nend\n\nfor epoch in 1:100\n    Flux.Optimise.train!(loss, ps, dataloader, opt)\nend\n\n# Finish the run\nclose(lg)","category":"page"},{"location":"examples/mpi/","page":"MPI.jl Integration","title":"MPI.jl Integration","text":"The main points when using MPI and Wandb are:","category":"page"},{"location":"examples/mpi/","page":"MPI.jl Integration","title":"MPI.jl Integration","text":"FluxMPI.Init() must be called before WandbLoggerMPI is called.\nThe config cannot be updated after WandbLoggerMPI is initialized, i.e. update_config! won't work.\nLogging is done the following manner:\nIf group kwarg is not passed/ is nothing: All the logging is done by the process with rank = 0.\nIf group is set to a string: Look at https://docs.wandb.ai/guides/track/advanced/grouping for more details. (The rank of the processes are appended to the name if set)","category":"page"},{"location":"examples/mpi/","page":"MPI.jl Integration","title":"MPI.jl Integration","text":"The code should be run using mpiexecjl -n 3 julia <script>.jl.","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"CurrentModule = Wandb","category":"page"},{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api/#Core-Functionalities","page":"API Reference","title":"Core Functionalities","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"For a lot of these functions, the official WanDB docs can provide more comprehensive details.","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"WandbLogger\nWandb.increment_step!\nupdate_config!\nget_config\nWandb.save\nWandb.version\nWandb.update_client\nWandb.log\nWandb.logable_propertynames\nBase.close","category":"page"},{"location":"api/#Wandb.WandbLogger","page":"API Reference","title":"Wandb.WandbLogger","text":"WandbLogger(; project, name=nothing, min_level=Info, step_increment=1,\n            start_step=0, kwargs...)\n\nCreate a WandbLogger that logs to the wandb project project. See the documentation for wandb.init for more details (also accessible in the Julia REPL using ? Wandb.wandb.init).\n\n\n\n\n\n","category":"type"},{"location":"api/#Wandb.increment_step!","page":"API Reference","title":"Wandb.increment_step!","text":"increment_step!(lg::WandbLogger, Δstep)\n\nIncrement the global step by Δstep and return the new global step.\n\n\n\n\n\n","category":"function"},{"location":"api/#Wandb.update_config!","page":"API Reference","title":"Wandb.update_config!","text":"update_config!(lg::WandbLogger, dict::Dict; kwargs...)\n\nFor more details checkout wandb.config (or ? Wandb.wandb.config in the Julia REPL).\n\n\n\n\n\n","category":"function"},{"location":"api/#Wandb.get_config","page":"API Reference","title":"Wandb.get_config","text":"get_config(lg::WandbLogger)\n\nReturn the current config dict.\n\n\n\n\n\nget_config(lg::WandbLogger, key::String)\n\nGet the value of key from the config.\n\n\n\n\n\n","category":"function"},{"location":"api/#Wandb.save","page":"API Reference","title":"Wandb.save","text":"save(lg::WandbLogger, args...; kwargs...)\n\nEnsure all files matching glob_str are synced to wandb with the policy specified. For more details checkout wandb.save (or ? Wandb.wandb.save in the Julia REPL).\n\n\n\n\n\n","category":"function"},{"location":"api/#Wandb.version","page":"API Reference","title":"Wandb.version","text":"version()\n\nReturn the Wandb python client version number (i.e., Wandb.wandb.__version__).\n\n\n\n\n\n","category":"function"},{"location":"api/#Wandb.update_client","page":"API Reference","title":"Wandb.update_client","text":"update_client()\n\nUpdates the python dependencies in Wandb.jl.\n\n\n\n\n\n","category":"function"},{"location":"api/#Wandb.log","page":"API Reference","title":"Wandb.log","text":"log(lg::WandbLogger, logs::Dict; kwargs...)\n\nFor more details checkout wandb.log (or ? Wandb.wandb.log in the Julia REPL).\n\n\n\n\n\n","category":"function"},{"location":"api/#Wandb.logable_propertynames","page":"API Reference","title":"Wandb.logable_propertynames","text":"logable_propertynames(val::Any)\n\nReturns a tuple with the name of the fields of the structure val that should be logged to Wandb. This function should be overridden when you want Wandb to ignore some fields in a structure when logging it. The default behaviour is to return the  same result as propertynames.\n\nSee also: Base.propertynames\n\n\n\n\n\n","category":"function"},{"location":"api/#Base.close","page":"API Reference","title":"Base.close","text":"close(lg::WandbLogger; kwargs...)\n\nTerminate the current run. For more details checkout wandb.finish (or ? Wandb.wandb.finish in the Julia REPL).\n\n\n\n\n\n","category":"function"},{"location":"api/#Plotting","page":"API Reference","title":"Plotting","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Wandb.plot_line\nWandb.plot_scatter\nWandb.plot_histogram\nWandb.plot_bar\nWandb.plot_line_series","category":"page"},{"location":"api/#Wandb.plot_line","page":"API Reference","title":"Wandb.plot_line","text":"plot_line(table::wandb.Table, x::String, y::String, stroke::String, title::String)\n\nConstruct a line plot.\n\nArguments:\n\ntable (wandb.Table): Table of data.\nx (string): Name of column to as for x-axis values.\ny (string): Name of column to as for y-axis values.\nstroke (string): Name of column to map to the line stroke scale.\ntitle (string): Plot title.\n\nReturns:\n\nA plot object, to be passed to Wandb.log()\n\n\n\n\n\n","category":"function"},{"location":"api/#Wandb.plot_scatter","page":"API Reference","title":"Wandb.plot_scatter","text":"plot_scatter(table::Py, x::String, y::String, title::String)\n\nConstruct a scatter plot.\n\nArguments:\n\ntable (wandb.Table): Table of data.\nx (string): Name of column to as for x-axis values.\ny (string): Name of column to as for y-axis values.\ntitle (string): Plot title.\n\nReturns:\n\nA plot object, to be passed to Wandb.log()\n\n\n\n\n\n","category":"function"},{"location":"api/#Wandb.plot_histogram","page":"API Reference","title":"Wandb.plot_histogram","text":"plot_histogram(table::Py, value::String, title::String)\n\nConstruct a histogram plot.\n\nArguments:\n\ntable (wandb.Table): Table of data.\nvalue (string): Name of column to use as data for bucketing.\ntitle (string): Plot title.\n\nReturns:\n\nA plot object, to be passed to Wandb.log()\n\n\n\n\n\n","category":"function"},{"location":"api/#Wandb.plot_bar","page":"API Reference","title":"Wandb.plot_bar","text":"plot_bar(table::Py, label::String, value::String, title::String)\n\nConstruct a bar plot.\n\nArguments:\n\ntable (wandb.Table): Table of data.\nlabel (string): Name of column to use as each bar's label.\nvalue (string): Name of column to use as each bar's value.\ntitle (string): Plot title.\n\nReturns:\n\nA plot object, to be passed to Wandb.log()\n\n\n\n\n\n","category":"function"},{"location":"api/#Wandb.plot_line_series","page":"API Reference","title":"Wandb.plot_line_series","text":"plot_line_series(xs::AbstractVecOrMat, ys::AbstractMatrix, keys::AbstractVector,\n                 title::String, xname::String)\n\nConstruct a line series plot.\n\nArguments:\n\nxs (AbstractMatrix or AbstractVector): Array of arrays of x values\nys (AbstractMatrix): Array of y values\nkeys (AbstractVector): Array of labels for the line plots\ntitle (string): Plot title.\nxname (string): Title of x-axis\n\nReturns:\n\nA plot object, to be passed to Wandb.log()\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API Reference","title":"API Reference","text":"note: Note\nWe don't provide direct bindings for the other plotting functions. PRs implementing these are welcome.","category":"page"},{"location":"api/#Loggable-Objects","page":"API Reference","title":"Loggable Objects","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Wandb.Image\nWandb.Video\nWandb.Histogram\nWandb.Object3D\nWandb.Table","category":"page"},{"location":"api/#Wandb.Image","page":"API Reference","title":"Wandb.Image","text":"Image(img::AbstractArray{T, 3}; kwargs...)\nImage(img::AbstractMatrix; kwargs...)\nImage(img::Union{String, IO}; kwargs...)\n\nCreates a wandb.Image object that can be logged using Wandb.log. For more details checkout wandb.Image (or ? Wandb.wandb.Image in the Julia REPL).\n\n\n\n\n\n","category":"function"},{"location":"api/#Wandb.Video","page":"API Reference","title":"Wandb.Video","text":"Video(vid::AbstractArray{T, 5}; kwargs...)\nVideo(vid::AbstractArray{T, 4}; kwargs...)\nVideo(vid::Union{String, IO}; kwargs...)\n\nCreates a wandb.Video object that can be logged using Wandb.log. For more details checkout wandb.Video (or ? Wandb.wandb.Video in the Julia REPL).\n\n\n\n\n\n","category":"function"},{"location":"api/#Wandb.Histogram","page":"API Reference","title":"Wandb.Histogram","text":"Wandb.Histogram(args...; kwargs...)\n\nCreates a wandb.Histogram object that can be logged using Wandb.log. For more details checkout wandb.Histogram (or ? Wandb.wandb.Histogram in the Julia REPL).\n\n\n\n\n\n","category":"function"},{"location":"api/#Wandb.Object3D","page":"API Reference","title":"Wandb.Object3D","text":"Object3D(path_or_io::Union{String, IO})\nObject3D(data::AbstractMatrix)\n\nCreates a wandb.Object3D object that can be logged using Wandb.log. For more details checkout wandb.Object3D (or ? Wandb.wandb.Object3D in the Julia REPL).\n\n\n\n\n\n","category":"function"},{"location":"api/#Wandb.Table","page":"API Reference","title":"Wandb.Table","text":"Table(; data::AbstractMatrix, columns::AbstractVector)\n\nCreates a wandb.Table object that can be logged using Wandb.log. For more details checkout wandb.Table (or ? Wandb.wandb.Table in the Julia REPL).\n\nnote: Note\nCurrently we don't support passing DataFrame to the Table. (PRs implementing this are welcome)\n\n\n\n\n\n","category":"function"},{"location":"api/#Artifacts","page":"API Reference","title":"Artifacts","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"WandbArtifact","category":"page"},{"location":"api/#Wandb.WandbArtifact","page":"API Reference","title":"Wandb.WandbArtifact","text":"WandbArtifact(args...; kwargs...)\n\nWandbArtifact is a wrapper around wandb.Artifact. See the documentation for wandb.Artifact for the different functionalities. Most of the functions can be called using Wandb.<function name>(::WandbArticact, args...; kwargs...).\n\n\n\n\n\n","category":"type"},{"location":"api/#Hyper-Parameter-Sweep","page":"API Reference","title":"Hyper Parameter Sweep","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"WandbHyperParameterSweep","category":"page"},{"location":"api/#Wandb.WandbHyperParameterSweep","page":"API Reference","title":"Wandb.WandbHyperParameterSweep","text":"WandbHyperParameterSweep()\n\nCreate a Wandb Hyperparameter Sweep. Unlike the wandb.agent API, this API needs to be manually combined with some Hyper Parameter Optimization package like HyperOpt.jl.\n\nSee the tutorials for more details.\n\n\n\n\n\n","category":"type"},{"location":"api/#Index","page":"API Reference","title":"Index","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Pages = [\"api.md\"]","category":"page"},{"location":"examples/artifacts/#Wandb-Artifacts","page":"Artifacts API","title":"Wandb Artifacts","text":"","category":"section"},{"location":"examples/artifacts/","page":"Artifacts API","title":"Artifacts API","text":"All the methods listed in https://docs.wandb.ai/ref/python/artifact can be used by passing a WandbArtifact instance as the first argument. Additionally, log should be used instead of log_artifact function.","category":"page"},{"location":"examples/artifacts/","page":"Artifacts API","title":"Artifacts API","text":"NOTE: These functions are not exported. So use them as Wandb.<function>(::WandbArtifact, ...)","category":"page"},{"location":"examples/artifacts/#Example-Usage:","page":"Artifacts API","title":"Example Usage:","text":"","category":"section"},{"location":"examples/artifacts/","page":"Artifacts API","title":"Artifacts API","text":"using Wandb\n\nlg = WandbLogger(project = \"Wandb.jl\")\nwa = WandbArtifact(\"some-dataset\", type = \"dataset\")\nWandb.add_file(wa, \"a.txt\")\n\nWandb.log(lg, wa)\n\nclose(lg)","category":"page"},{"location":"examples/artifacts/","page":"Artifacts API","title":"Artifacts API","text":"When uploading large artifacts it might be difficult to verify if the files are actually being uploaded or if the code is just stuck. Run the following in your terminal replacing $WANDB_DIR with the path to the local wandb directory:","category":"page"},{"location":"examples/artifacts/","page":"Artifacts API","title":"Artifacts API","text":"tail -f $WANDB_DIR/latest_run/debug-internal.log","category":"page"},{"location":"examples/artifacts/","page":"Artifacts API","title":"Artifacts API","text":"If you have another wandb run started after this, you need to modify the path accordingly.","category":"page"},{"location":"examples/fluxtraining/#Integration-with-FluxTraining.jl","page":"FluxTraining.jl Integration","title":"Integration with FluxTraining.jl","text":"","category":"section"},{"location":"examples/fluxtraining/","page":"FluxTraining.jl Integration","title":"FluxTraining.jl Integration","text":"We have bindings in the form of WandbBackend which can be used as a dropin replacement for the default TensorboardBackend. Just ensure that FluxTraining.jl is installed and loaded explicitly.","category":"page"},{"location":"examples/fluxtraining/","page":"FluxTraining.jl Integration","title":"FluxTraining.jl Integration","text":"Let's go through the FluxTraining.jl MNIST example and update it to use Wandb.","category":"page"},{"location":"examples/fluxtraining/","page":"FluxTraining.jl Integration","title":"FluxTraining.jl Integration","text":"using Flux, FluxTraining, Wandb, Dates\nusing MLUtils: splitobs, unsqueeze\nusing MLDatasets: MNIST\nusing Flux: onehotbatch\nusing Flux.Data: DataLoader\n\n# import data\ndata = MNIST(:train)[:]\n\nconst LABELS = 0:9\n\n# unsqueeze to reshape from (28, 28, numobs) to (28, 28, 1, numobs)\npreprocess((data, targets)) = unsqueeze(data, 3), onehotbatch(targets, LABELS)\n\n# traindata and testdata contain both inputs (pixel values) and targets (correct labels)\ntraindata = MNIST(Float32, :train)[:] |> preprocess\ntestdata = MNIST(Float32, :test)[:] |> preprocess\n\n# create iterators\ntrainiter, testiter = DataLoader(traindata, batchsize=128), DataLoader(testdata, batchsize=256);\n\n# create model and optimizer\nmodel = Chain(Conv((3, 3), 1 => 16, relu, pad = 1, stride = 2),\n              Conv((3, 3), 16 => 32, relu, pad = 1), GlobalMeanPool(), Flux.flatten,\n              Dense(32, 10))\nlossfn = Flux.Losses.logitcrossentropy\noptimizer = Flux.ADAM();\n\n# prepare learner\nlogwandb = LogMetrics(WandbBackend(project = \"Wandb.jl\", \n                                    name = \"fluxtrainingjl-integration-$(now())\"))\nlearner = Learner(model, lossfn; callbacks=[ToGPU(), Metrics(accuracy), logwandb],\n                  optimizer)\n\n# fit model\nFluxTraining.fit!(learner, 10, (trainiter, testiter))","category":"page"},{"location":"examples/hparams/#Hyperparameter-Sweeps","page":"HyperParameter Sweeps","title":"Hyperparameter Sweeps","text":"","category":"section"},{"location":"examples/hparams/","page":"HyperParameter Sweeps","title":"HyperParameter Sweeps","text":"We currently don't support Wandb Agents API since it leads to segfaults. Instead we recommend users to install Hyperopt.jl or any other HyperParameter Optimization Library. The resultant Wandb logs aren't as neat as the official sweeps but do get the job done.","category":"page"},{"location":"examples/hparams/","page":"HyperParameter Sweeps","title":"HyperParameter Sweeps","text":"using Hyperopt, Wandb\n\nfunction f(x, a, b; c)\n  return sum(@. x + (a - 3) ^ 2 + (b ? 10 : 20) + (c - 100) ^ 2) # Function to minimize\nend\n\n# This function dispatch must be present\n# `lg` can be `WandbBackend` if using that with HyperParameter Sweep\nfunction f(lg::WandbLogger, config)\n  res = f(config[\"x\"], config[\"a\"], config[\"b\"]; c = config[\"c\"])\n  Wandb.log(lg, Dict(\"result\" => res))\n  return res\nend\n\nhpsweep = WandbHyperParameterSweep()\n\n# Main macro. The first argument to the for loop is always interpreted as the number of iterations\nho = @hyperopt for i = 50, sampler = RandomSampler(), # This is default if none provided\n                   a = LinRange(1,5,1000), b = [true, false],\n                   c = exp10.(LinRange(-1,3,1000))\n    hpsweep(f, Dict(\"a\" => a, \"b\" => b, \"c\" => c), project = \"Wandb.jl\",\n            config = Dict(\"x\" => 100))\nend","category":"page"},{"location":"examples/hparams/","page":"HyperParameter Sweeps","title":"HyperParameter Sweeps","text":"After this is done, we need to do some manual tweaking in the Wandb UI to get a clean visualization. First, filter the runs using the tag in hpsweep. Then just add a Parallel Coordinates plot with the hyperparameters.","category":"page"},{"location":"examples/hparams/","page":"HyperParameter Sweeps","title":"HyperParameter Sweeps","text":"(Image: Parallel Coordinates Plot)","category":"page"},{"location":"examples/demo/#Getting-Started-Tutorial","page":"Getting Started","title":"Getting Started Tutorial","text":"","category":"section"},{"location":"examples/demo/","page":"Getting Started","title":"Getting Started","text":"Example borrowed from here","category":"page"},{"location":"examples/demo/","page":"Getting Started","title":"Getting Started","text":"using Wandb, Dates, Logging\n\n# Start a new run, tracking hyperparameters in config\nlg = WandbLogger(project = \"Wandb.jl\", name = \"wandbjl-demo-$(now())\",\n                 config = Dict(\"learning_rate\" => 0.01, \"dropout\" => 0.2,\n                               \"architecture\" => \"CNN\", \"dataset\" => \"CIFAR-100\"))\n\n# Use LoggingExtras.jl to log to multiple loggers together\nglobal_logger(lg)\n\n# Simulating the training or evaluation loop\nfor x in 1:50\n  acc = log(1 + x + rand() * get_config(lg, \"learning_rate\") + rand() + \n            get_config(lg, \"dropout\"))\n  loss = 10 - log(1 + x + rand() + x * get_config(lg, \"learning_rate\") + rand() +\n                  get_config(lg, \"dropout\"))\n  # Log metrics from your script to W&B\n  @info \"metrics\" accuracy=acc loss=loss\nend\n\n# Finish the run\nclose(lg)","category":"page"},{"location":"examples/flux/#Integration-with-Flux.jl","page":"Flux.jl Intergration","title":"Integration with Flux.jl","text":"","category":"section"},{"location":"examples/flux/","page":"Flux.jl Intergration","title":"Flux.jl Intergration","text":"Using Wandb.jl in existing Flux workflows is pretty easy. Let's go through the mp_mnist demo in Flux model-zoo and update it to use Wandb. Firstly, use this evironment and add Wandb.jl to it.","category":"page"},{"location":"examples/flux/","page":"Flux.jl Intergration","title":"Flux.jl Intergration","text":"using Flux, Statistics\nusing Flux.Data: DataLoader\nusing Flux: onehotbatch, onecold, @epochs\nusing Flux.Losses: logitcrossentropy\nusing CUDA\nusing MLDatasets\nusing Wandb\nusing Dates\nusing Logging\n\nlg = WandbLogger(project = \"Wandb.jl\", name = \"fluxjl-integration-$(now())\",\n                 config = Dict(\"learning_rate\" => 3e-4, \"batchsize\" => 256,\n                               \"epochs\" => 100, \"dataset\" => \"MNIST\", \"use_cuda\" => true))\n\nglobal_logger(lg)\n\n##################################################################################\n# Wandb # Instead of passing arguments around we will use the global configuration\n# Wandb # file from Wandb\n##################################################################################\nfunction getdata(device)\n    ENV[\"DATADEPS_ALWAYS_ACCEPT\"] = \"true\"\n\n    # Loading Dataset\t\n    xtrain, ytrain = MLDatasets.MNIST.traindata(Float32)\n    xtest, ytest = MLDatasets.MNIST.testdata(Float32)\n\n    # Reshape Data in order to flatten each image into a linear array\n    xtrain = Flux.flatten(xtrain)\n    xtest = Flux.flatten(xtest)\n\n    # One-hot-encode the labels\n    ytrain, ytest = onehotbatch(ytrain, 0:9), onehotbatch(ytest, 0:9)\n\n    # Create DataLoaders (mini-batch iterators)\n    train_loader = DataLoader(\n        (xtrain, ytrain),\n        batchsize = get_config(lg, \"batchsize\"),\n        shuffle = true,\n    )\n    test_loader = DataLoader((xtest, ytest), batchsize = get_config(lg, \"batchsize\"))\n\n    return train_loader, test_loader\nend\n\nbuild_model(; imgsize = (28, 28, 1), nclasses = 10) =\n    Chain(Dense(prod(imgsize), 32, relu), Dense(32, nclasses))\n\nfunction loss_and_accuracy(data_loader, model, device)\n    acc = 0\n    ls = 0.0f0\n    num = 0\n    for (x, y) in data_loader\n        x, y = device(x), device(y)\n        ŷ = model(x)\n        ls += logitcrossentropy(model(x), y, agg = sum)\n        acc += sum(onecold(cpu(model(x))) .== onecold(cpu(y)))\n        num += size(x, 2)\n    end\n    return ls / num, acc / num\nend\n\n#################################################################\n# Wandb # If any paramters need to be updated pass them as a Dict\n#################################################################\nfunction train(update_params::Dict = Dict())\n    #################################\n    # Wandb # Update config if needed\n    #################################\n    update_config!(lg, update_params)\n\n    if CUDA.functional() && get_config(lg, \"use_cuda\")\n        @info \"Training on CUDA GPU\"\n        CUDA.allowscalar(false)\n        device = gpu\n    else\n        @info \"Training on CPU\"\n        device = cpu\n    end\n\n    # Create test and train dataloaders\n    train_loader, test_loader = getdata(device)\n\n    # Construct model\n    model = build_model() |> device\n    ps = Flux.params(model) # model's trainable parameters\n\n    ## Optimizer\n    opt = ADAM(get_config(lg, \"learning_rate\"))\n\n    ## Training\n    for epoch = 1:get_config(lg, \"epochs\")\n        for (x, y) in train_loader\n            x, y = device(x), device(y) # transfer data to device\n            gs = gradient(() -> logitcrossentropy(model(x), y), ps) # compute gradient\n            Flux.Optimise.update!(opt, ps, gs) # update parameters\n        end\n\n        # Report on train and test\n        train_loss, train_acc = loss_and_accuracy(train_loader, model, device)\n        test_loss, test_acc = loss_and_accuracy(test_loader, model, device)\n\n        ###################################\n        # Wandb # Log the loss and accuracy\n        ###################################\n        Wandb.log(\n            lg,\n            Dict(\n                \"Training/Loss\" => train_loss,\n                \"Training/Accuracy\" => train_acc,\n                \"Testing/Loss\" => test_loss,\n                \"Testing/Accuracy\" => test_acc,\n            ),\n        )\n\n        println(\"Epoch=$epoch\")\n        println(\"  train_loss = $train_loss, train_accuracy = $train_acc\")\n        println(\"  test_loss = $test_loss, test_accuracy = $test_acc\")\n    end\nend\n\n### Run training \ntrain()\n\n################################\n# Wandb # Finish the Current Run\n################################\nclose(lg)","category":"page"},{"location":"#Wandb.jl","page":"Home","title":"Wandb.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Unofficial Julia Bindings for wandb.ai.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"For stable release:","category":"page"},{"location":"","page":"Home","title":"Home","text":"] add Wandb","category":"page"},{"location":"","page":"Home","title":"Home","text":"For the main branch:","category":"page"},{"location":"","page":"Home","title":"Home","text":"] add Wandb#main","category":"page"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Follow the quickstart points 1 and 2 to get started with a Wandb account.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Wandb, Logging\n\n# Initialize the project\nlg = WandbLogger(; project = \"Wandb.jl\", name = nothing)\n\n# Set logger globally / in scope / in combination with other loggers\nglobal_logger(lg)\n\n# Logging Values\nWandb.log(lg, Dict(\"accuracy\" => 0.9, \"loss\" => 0.3))\n\n# Even more conveniently\n@info \"metrics\" accuracy=0.9 loss=0.3\n@debug \"metrics\" not_print=-1  # Will have to change debug level for this to be logged\n\n# Tracking Hyperparameters\nupdate_config!(lg, Dict(\"dropout\" => 0.2))\n\n# Close the logger\nclose(lg)","category":"page"},{"location":"#Examples","page":"Home","title":"Examples","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To see the logging in action go here. Detailed code for these examples can be accessed via the navigation menu.","category":"page"},{"location":"#Running-into-Issues","page":"Home","title":"Running into Issues","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Please have a look at the Miscellaneous Section to see if it solves your issue. If not, please report bugs using GitHub Issues. For usage questions post them on Discourse (@avik-pal) or Julia Slack (#helpdesk channel) (@avikpal) tagging me.","category":"page"}]
}
